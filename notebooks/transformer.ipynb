{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Подготовка данных\n",
    "data = pd.read_csv('path/to/your/data.csv')\n",
    "articles = pd.read_csv('path/to/articles.csv')\n",
    "\n",
    "# Преобразование данных в формат последовательностей\n",
    "sequences = data.groupby('customer_id')['article_id'].apply(list).reset_index()\n",
    "\n",
    "# 2. Создание датасета\n",
    "class RecommendationDataset(Dataset):\n",
    "    def __init__(self, sequences, max_len):\n",
    "        self.sequences = sequences\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_seq = sequence[:-1]\n",
    "        target_seq = sequence[1:]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "\n",
    "max_len = 50  # Максимальная длина последовательности\n",
    "train_sequences, test_sequences = train_test_split(sequences['article_id'].tolist(), test_size=0.2)\n",
    "train_dataset = RecommendationDataset(train_sequences, max_len)\n",
    "test_dataset = RecommendationDataset(test_sequences, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 3. Создание модели Transformer\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_len):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) * math.sqrt(self.max_len)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.max_len)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "vocab_size = len(articles['article_id'].unique())\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_len)\n",
    "\n",
    "# 4. Обучение модели\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for input_seq, target_seq in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq, target_seq)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_seq.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# 5. Оценка модели\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for input_seq, target_seq in test_loader:\n",
    "        output = model(input_seq, target_seq)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_seq.view(-1))\n",
    "        total_loss += loss.item()\n",
    "print(f'Test Loss: {total_loss / len(test_loader)}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
