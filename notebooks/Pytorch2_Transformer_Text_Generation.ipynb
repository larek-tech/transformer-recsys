{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e410aa",
   "metadata": {},
   "source": [
    "# Transformer, many-to-many with text\n",
    "### Decoder Only Architecture\n",
    "\n",
    "Let's revisit text generation and see how we can use Attention to create a Transformer instead of an LSTM. The type of Transformer network we will be using is called a \"Decoder only\" as there is no cross-attention.\n",
    "We will instead be using a type of Self-Attention called \"Masked Self-Attention\". Similar to regular Self-Attention, \"Masked Self-Attention\" uses \"Causal Masking\" to prevent tokens from querying other tokens that are later in the input sequence.<br>\n",
    "Why?\n",
    "<br>\n",
    "As we're doing next-token prediction, if we let a token query every token in the sequence it will be able to simply \"look\" at what comes next in the sequence and return exactly what comes next! This is not useful as at test time when we want it to generate text it won't be able to do that!<br>\n",
    "With \"Causal Masking\" we simply mask-out (multiply by zero) regions of the attention map that correspond to a token querying tokens that are later in the sequence. As a result a token will only be able to query itself, or any token that came BEFORE it in the sequence!\n",
    "\n",
    "<img src=\"../data/llm_architecture_comparison.png\" width=\"600\" align=\"center\">\n",
    "<br>\n",
    "We don't be exactly implementing the Decoder only Transformer detailed above, our network will basically be the Decoder from the Encoder-Decoder network without the Cross-Attention (input from the Encoder).\n",
    "<br>\n",
    "NOTE: We will cover Encoder-Decoder networks in the next notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaab1d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-17 22:32:49.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrecsys.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /home/artem216/transformer-recsys\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from torchtext.datasets import WikiText2, EnWik9, AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchtext.data.functional import sentencepiece_tokenizer, load_sp_model\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "import ast\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from recsys import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5e339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Number of epochs for training\n",
    "nepochs = 10\n",
    "\n",
    "# Batch size for data loaders\n",
    "batch_size = 4096\n",
    "\n",
    "# Maximum sequence length for text inputs\n",
    "max_len = 17\n",
    "\n",
    "# Root directory of the dataset\n",
    "data_set_root = \"../../datasets\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e824fd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(config.PROCESSED_DATA_DIR / \"x_train_ids.csv\")\n",
    "df_test = pd.read_csv(config.PROCESSED_DATA_DIR / \"x_test_ids.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30516eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_articles_train = set()\n",
    "unique_articles_test = set()\n",
    "\n",
    "for articles in df_train[\"articles\"]:\n",
    "    if isinstance(articles, str):\n",
    "        articles = ast.literal_eval(articles)\n",
    "    unique_articles_train.update(articles)\n",
    "\n",
    "for articles in df_test[\"articles\"]:\n",
    "    if isinstance(articles, str):\n",
    "        articles = ast.literal_eval(articles)\n",
    "    unique_articles_test.update(articles)\n",
    "\n",
    "unique_articles = unique_articles_train.union(unique_articles_test) \n",
    "\n",
    "article_id_map = {article_id: idx for idx, article_id in enumerate(unique_articles)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803a8dc",
   "metadata": {},
   "source": [
    "## Data processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e1f4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerDataset(Dataset):\n",
    "    def __init__(self, df, max_len, article_id_map):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.article_id_map = article_id_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        articles = self.df.iloc[idx][\"articles\"]\n",
    "        if isinstance(articles, str):\n",
    "            articles = ast.literal_eval(\n",
    "                articles\n",
    "            )\n",
    "        articles = [\n",
    "            self.article_id_map[article] for article in articles\n",
    "        ]\n",
    "        # TODO: <sos> token\n",
    "        articles = articles + [0] * (self.max_len - len(articles))  # Padding\n",
    "        input_seq = torch.tensor(articles[:-1], dtype=torch.long)\n",
    "        target_seq = torch.tensor(articles[1:], dtype=torch.long) # 1:\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7087111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing datasets\n",
    "df_train = pd.read_csv(config.PROCESSED_DATA_DIR / \"x_train_ids.csv\")\n",
    "df_test = pd.read_csv(config.PROCESSED_DATA_DIR / \"x_test_ids.csv\")\n",
    "# Create data loaders for the training and testing datasets\n",
    "dataset_train = CustomerDataset(df_train, max_len, article_id_map)\n",
    "dataset_test = CustomerDataset(df_test, max_len, article_id_map)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a148c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fff8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\"For a batch of tokens indices, randomly replace a non-specical token with <pad>.\n",
    "    \n",
    "    Args:\n",
    "        prob (float): probability of dropping a token\n",
    "        pad_token (int): index for the <pad> token\n",
    "        num_special (int): Number of special tokens, assumed to be at the start of the vocab\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n",
    "        self.prob = prob\n",
    "        self.num_special = num_special\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Randomly sample a bernoulli distribution with p=prob\n",
    "        # to create a mask where 1 means we will replace that token\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        \n",
    "        # only replace if the token is not a special token\n",
    "        can_drop = (sample >= self.num_special).long()\n",
    "        mask = mask * can_drop\n",
    "        \n",
    "        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n",
    "        \n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        \n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca059c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tranform = T.Sequential(\n",
    "#     # Tokeniz with pre-existing Tokenizer\n",
    "#     T.SentencePieceTokenizer(\"spm_ag_news.model\"),\n",
    "#     ## converts the sentences to indices based on given vocabulary\n",
    "#     T.VocabTransform(vocab=vocab),\n",
    "#     ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "#     # 1 as seen in previous section\n",
    "#     T.AddToken(1, begin=True),\n",
    "#     # Crop the sentance if it is longer than the max length\n",
    "#     T.Truncate(max_seq_len=max_len),\n",
    "#     ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "#     # 2 as seen in previous section\n",
    "#     T.AddToken(2, begin=False),\n",
    "#     # Convert the list of lists to a tensor, this will also\n",
    "#     # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "#     # This ensures all sentences are the same length!\n",
    "#     T.ToTensor(padding_value=0),\n",
    "# )\n",
    "\n",
    "# gen_tranform = T.Sequential(\n",
    "#     # Tokeniz with pre-existing Tokenizer\n",
    "#     T.SentencePieceTokenizer(\"spm_ag_news.model\"),\n",
    "#     ## converts the sentences to indices based on given vocabulary\n",
    "#     T.VocabTransform(vocab=vocab),\n",
    "#     ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "#     # 1 as seen in previous section\n",
    "#     T.AddToken(1, begin=True),\n",
    "#     # Convert the list of lists to a tensor, this will also\n",
    "#     # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "#     # This ensures all sentences are the same length!\n",
    "#     T.ToTensor(padding_value=0)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b6eba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = next(iter(data_loader_train))\n",
    "# index = 0\n",
    "# input_tokens = train_tranform(list(text))\n",
    "# print(\"SENTENCE\")\n",
    "# print(text[index])\n",
    "# print()\n",
    "# print(\"TOKENS\")\n",
    "# print(vocab.lookup_tokens(input_tokens[index].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68178f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"TOKENS BACK TO SENTENCE\")\n",
    "\n",
    "# pred_text = \"\".join(vocab.lookup_tokens(input_tokens[index].numpy()))\n",
    "# pred_text.replace(\"‚ñÅ\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65424c",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "effad54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal positional embeddings\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional embeddings module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate sinusoidal positional embeddings\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "    \n",
    "# Transformer block with Attention and causal masking\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with self-attention and causal masking.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=128, num_heads=4):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # Layer normalization for input\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Multi-head self-attention mechanism\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, \n",
    "                                                    num_heads=num_heads, \n",
    "                                                    batch_first=True,\n",
    "                                                    dropout=0.1)\n",
    "\n",
    "        # Layer normalization for attention output\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Feedforward neural network\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, padding_mask):\n",
    "        # Create causal mask for Attention\n",
    "        bs, l, h = x.shape\n",
    "        mask = torch.triu(torch.ones(l, l, device=x.device), 1).bool()\n",
    "\n",
    "        # Layer normalization\n",
    "        norm_x = self.norm1(x)\n",
    "\n",
    "        # Apply multi-head Attention\n",
    "        x = self.multihead_attn(norm_x, norm_x, norm_x, attn_mask=mask, key_padding_mask=padding_mask)[0] + x\n",
    "\n",
    "        # Layer normalization\n",
    "        norm_x = self.norm2(x)\n",
    "\n",
    "        # Apply feedforward neural network\n",
    "        x = self.mlp(norm_x) + x\n",
    "        return x\n",
    "\n",
    "    \n",
    "# \"Decoder-Only\" Style Transformer with Attention\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    \"Decoder-Only\" Style Transformer with self-attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Token embeddings\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "\n",
    "        # Positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "\n",
    "        # List of Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Mask for padding tokens\n",
    "        input_key_mask = input_seq == 0\n",
    "\n",
    "        # Embedding input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to token embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "\n",
    "        # Pass through Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs, padding_mask=input_key_mask)\n",
    "\n",
    "        # Output predictions\n",
    "        return self.fc_out(embs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc39c75",
   "metadata": {},
   "source": [
    "## Initialise Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f720e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Embedding Size\n",
    "hidden_size = 256\n",
    "\n",
    "# Number of transformer blocks\n",
    "num_layers = 8\n",
    "\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8\n",
    "\n",
    "num_embeding = len(unique_articles) + 1\n",
    "# Create model\n",
    "tf_generator = Transformer(num_emb=num_embeding, num_layers=num_layers, \n",
    "                           hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(tf_generator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "# Custom transform that will randomly replace a token with <pad>\n",
    "td = TokenDrop(prob=0.2)\n",
    "\n",
    "# Initialize training loss logger and entropy logger\n",
    "training_loss_logger = []\n",
    "entropy_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fcba694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-This Model Has 6831593 (Approximately 6 Million) Parameters!\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in tf_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8b005",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70ae0201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e9da7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac52cc6b216848c3a4f90bbf8d214da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7718d7886e28429d8a8966d81ff68ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tf_generator\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Convert text to tokenized input\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# text_tokens = train_tranform(list(text)).to(device)\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# bs = text_tokens.shape[0]\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# # Randomly drop input tokens\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# input_text = td(text_tokens[:, 0:-1])\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# output_text = text_tokens[:, 1:]\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1085\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1083\u001b[0m _utils\u001b[38;5;241m.\u001b[39msignal_handling\u001b[38;5;241m.\u001b[39m_set_SIGCHLD_handler()\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_pids_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1118\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;66;03m# prime the prefetch loop\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers):\n\u001b[0;32m-> 1118\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_put_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1352\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_put_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1352\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:621\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/torch/utils/data/sampler.py:280\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msampler_iter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/torch/utils/data/sampler.py:280\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m         batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msampler_iter\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)]\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/torch/utils/data/sampler.py:153\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 153\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m         seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mempty((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mrandom_()\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mCustomerDataset.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf)\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mCustomerDataset.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf)\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1368\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1311\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2185\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2182\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2185\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2187\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2254\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2252\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2254\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2255\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):    \n",
    "    tf_generator.train()\n",
    "    steps = 0\n",
    "    for index, text in enumerate(tqdm(data_loader_train)):\n",
    "        # Convert text to tokenized input\n",
    "        # text_tokens = train_tranform(list(text)).to(device)\n",
    "        # bs = text_tokens.shape[0]\n",
    "        \n",
    "        # # Randomly drop input tokens\n",
    "        # input_text = td(text_tokens[:, 0:-1])\n",
    "        # output_text = text_tokens[:, 1:]\n",
    "        input_text, output_text = text\n",
    "        input_text = input_text.to(device)\n",
    "        output_text = output_text.to(device)\n",
    "        # Generate predictions\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = tf_generator(input_text)\n",
    "        \n",
    "        print(input_text)\n",
    "        input()\n",
    "\n",
    "        # Calculate loss with masked cross-entropy\n",
    "        mask = (output_text != 0).float()\n",
    "        loss = (loss_fn(pred.transpose(1, 2), output_text) * mask).sum()/mask.sum()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if index % 20 == 0:\n",
    "            print(\"train_loss\", loss, end=\"\\r\")\n",
    "        # Log training loss and entropy\n",
    "        # training_loss_logger.append(loss.item())\n",
    "        # with torch.no_grad():\n",
    "        #     dist = Categorical(logits=pred)\n",
    "        #     entropy_logger.append(dist.entropy().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82814571",
   "metadata": {},
   "source": [
    "## Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c93202a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAHDCAYAAAATEUquAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKtBJREFUeJzt3X2YlXWd+PHPwDADijMjj+PoIEpuoJjsguC4v5aK2dDYTYwulUsUiCsz8aEgVzGEtFpSc1PzgXV3W9YHgjCzldQiyLIcERBNEIg2FYVm8IkZfGBA5v79YZwaGfiCzWFQXq/rOped+3y/9/necF+j7+5z7inIsiwLAAAAdqldWy8AAABgfyecAAAAEoQTAABAgnACAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgAThBAAAkCCcANivjBs3Lnr37v2e5n7ta1+LgoKC1l0QAIRwAmAPFRQU7NHj4Ycfbuultolx48ZF586d23oZAORJQZZlWVsvAoD931133dXs+R133BELFiyIO++8s9n2f/zHf4yePXu+5/fZtm1bNDU1RXFx8V7Pffvtt+Ptt9+Ojh07vuf3f6/GjRsX99xzT7z++uv7/L0ByL/Ctl4AAO8PY8aMafb8scceiwULFuy0/d3efPPNOOigg/b4fTp06PCe1hcRUVhYGIWF/tUGQOvzUT0AWs3HPvax6N+/fyxbtiz+4R/+IQ466KC44oorIiLixz/+cYwYMSIqKiqiuLg4+vTpE1//+tdj+/btzfbx7u84Pffcc1FQUBDf/va34/bbb48+ffpEcXFxnHjiibFkyZJmc1v6jlNBQUFceOGFcd9990X//v2juLg4jjvuuHjooYd2Wv/DDz8cgwYNio4dO0afPn3i3//931v9e1Pz5s2LgQMHRqdOnaJbt24xZsyYWL9+fbMxtbW1MX78+DjiiCOiuLg4DjvssDjttNPiueeey41ZunRpDB8+PLp16xadOnWKo446Kj73uc+12joBaM7/LQdAq3rllVfi1FNPjbPOOivGjBmT+9jerFmzonPnzjFp0qTo3LlzLFq0KKZNmxYNDQ1x3XXXJfc7e/bs2Lx5c3zhC1+IgoKCuPbaa+Mzn/lM/OEPf0hepfr1r38d9957b1xwwQVxyCGHxE033RSjRo2KdevWRdeuXSMiYvny5XHKKafEYYcdFldddVVs3749rr766ujevftf/4fyJ7NmzYrx48fHiSeeGDNmzIi6urq48cYb4ze/+U0sX748ysrKIiJi1KhRsXLlyrjooouid+/esXHjxliwYEGsW7cu9/yTn/xkdO/ePS6//PIoKyuL5557Lu69995WWysA75IBwHswceLE7N3/Ghk6dGgWEdnMmTN3Gv/mm2/utO0LX/hCdtBBB2VbtmzJbRs7dmx25JFH5p4/++yzWURkXbt2zV599dXc9h//+MdZRGT3339/btv06dN3WlNEZEVFRdnvf//73Lannnoqi4jsu9/9bm7bP//zP2cHHXRQtn79+ty2tWvXZoWFhTvtsyVjx47NDj744F2+vnXr1qxHjx5Z//79s7feeiu3ff78+VlEZNOmTcuyLMtee+21LCKy6667bpf7+tGPfpRFRLZkyZLkugBoHT6qB0CrKi4ujvHjx++0vVOnTrn/vXnz5nj55Zfjox/9aLz55puxevXq5H7PPPPMOPTQQ3PPP/rRj0ZExB/+8Ifk3Orq6ujTp0/u+Uc+8pEoKSnJzd2+fXv8/Oc/j5EjR0ZFRUVu3Ic+9KE49dRTk/vfE0uXLo2NGzfGBRdc0OzmFSNGjIi+ffvGT37yk4h458+pqKgoHn744Xjttdda3NeOK1Pz58+Pbdu2tcr6ANg94QRAqzr88MOjqKhop+0rV66M008/PUpLS6OkpCS6d++eu7FEfX19cr+9evVq9nxHRO0qLnY3d8f8HXM3btwYb731VnzoQx/aaVxL296L559/PiIiPvzhD+/0Wt++fXOvFxcXxzXXXBMPPvhg9OzZM/7hH/4hrr322qitrc2NHzp0aIwaNSquuuqq6NatW5x22mnx3//939HY2NgqawVgZ8IJgFb1l1eWdti0aVMMHTo0nnrqqbj66qvj/vvvjwULFsQ111wTERFNTU3J/bZv377F7dke/FaNv2ZuW/jSl74Uv/vd72LGjBnRsWPHuPLKK6Nfv36xfPnyiHjnhhf33HNP1NTUxIUXXhjr16+Pz33uczFw4EC3QwfIE+EEQN49/PDD8corr8SsWbPikksuiX/6p3+K6urqZh+9a0s9evSIjh07xu9///udXmtp23tx5JFHRkTEmjVrdnptzZo1udd36NOnT0yePDl+9rOfxYoVK2Lr1q1x/fXXNxtz0kknxTe/+c1YunRp3H333bFy5cqYM2dOq6wXgOaEEwB5t+OKz19e4dm6dWvceuutbbWkZtq3bx/V1dVx3333xYYNG3Lbf//738eDDz7YKu8xaNCg6NGjR8ycObPZR+oefPDBWLVqVYwYMSIi3vm9V1u2bGk2t0+fPnHIIYfk5r322ms7XS0bMGBARISP6wHkiduRA5B3J598chx66KExduzYuPjii6OgoCDuvPPO/eqjcl/72tfiZz/7Wfz93/99fPGLX4zt27fHzTffHP37948nn3xyj/axbdu2+MY3vrHT9i5dusQFF1wQ11xzTYwfPz6GDh0ao0ePzt2OvHfv3vHlL385IiJ+97vfxbBhw+KMM86IY489NgoLC+NHP/pR1NXVxVlnnRUREf/zP/8Tt956a5x++unRp0+f2Lx5c/zHf/xHlJSUxKc+9alW+zMB4M+EEwB517Vr15g/f35Mnjw5pk6dGoceemiMGTMmhg0bFsOHD2/r5UVExMCBA+PBBx+Mr3zlK3HllVdGZWVlXH311bFq1ao9uutfxDtX0a688sqdtvfp0ycuuOCCGDduXBx00EHxrW99Ky677LI4+OCD4/TTT49rrrkmd6e8ysrKGD16dCxcuDDuvPPOKCwsjL59+8YPfvCDGDVqVES8c3OIxx9/PObMmRN1dXVRWloagwcPjrvvvjuOOuqoVvszAeDPCrL96f/uA4D9zMiRI2PlypWxdu3atl4KAG3Id5wA4E/eeuutZs/Xrl0bDzzwQHzsYx9rmwUBsN9wxQkA/uSwww6LcePGxdFHHx3PP/983HbbbdHY2BjLly+PY445pq2XB0Ab8h0nAPiTU045Jb7//e9HbW1tFBcXR1VVVfzrv/6raALAFScAAIAU33ECAABIEE4AAAAJB+R3nJqammLDhg1xyCGHREFBQVsvBwAAaCNZlsXmzZujoqIi2rXb9XWlAzKcNmzYEJWVlW29DAAAYD/xwgsvxBFHHLHL1w/IcDrkkEMi4p0/nJKSkjZeDQAA0FYaGhqisrIy1wi7ckCG046P55WUlAgnAAAg+RUeN4cAAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgAThBAAAkCCcAAAAEoQTAABAgnACAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgAThBAAAkCCcAAAAEoQTAABAgnACAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgAThBAAAkCCcAAAAEoQTAABAgnACAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgAThBAAAkCCcAAAAEoQTAABAgnACAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgAThBAAAkCCcAAAAEoQTAABAgnACAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgIR9Ek633HJL9O7dOzp27BhDhgyJxx9/fLfj582bF3379o2OHTvG8ccfHw888MAux55//vlRUFAQN9xwQyuvGgAA4B15D6e5c+fGpEmTYvr06fHEE0/ECSecEMOHD4+NGze2OP7RRx+N0aNHx4QJE2L58uUxcuTIGDlyZKxYsWKnsT/60Y/isccei4qKinwfBgAAcADLezj927/9W3z+85+P8ePHx7HHHhszZ86Mgw46KL73ve+1OP7GG2+MU045JS699NLo169ffP3rX4+/+7u/i5tvvrnZuPXr18dFF10Ud999d3To0CHfhwEAABzA8hpOW7dujWXLlkV1dfWf37Bdu6iuro6ampoW59TU1DQbHxExfPjwZuObmprinHPOiUsvvTSOO+64/CweAADgTwrzufOXX345tm/fHj179my2vWfPnrF69eoW59TW1rY4vra2Nvf8mmuuicLCwrj44ov3aB2NjY3R2NiYe97Q0LCnhwAAAPD+u6vesmXL4sYbb4xZs2ZFQUHBHs2ZMWNGlJaW5h6VlZV5XiUAAPBBktdw6tatW7Rv3z7q6uqaba+rq4vy8vIW55SXl+92/COPPBIbN26MXr16RWFhYRQWFsbzzz8fkydPjt69e7e4zylTpkR9fX3u8cILL/z1BwcAABww8hpORUVFMXDgwFi4cGFuW1NTUyxcuDCqqqpanFNVVdVsfETEggULcuPPOeec+O1vfxtPPvlk7lFRURGXXnpp/PSnP21xn8XFxVFSUtLsAQAAsKfy+h2niIhJkybF2LFjY9CgQTF48OC44YYb4o033ojx48dHRMS5554bhx9+eMyYMSMiIi655JIYOnRoXH/99TFixIiYM2dOLF26NG6//faIiOjatWt07dq12Xt06NAhysvL48Mf/nC+DwcAADgA5T2czjzzzHjppZdi2rRpUVtbGwMGDIiHHnoodwOIdevWRbt2f77wdfLJJ8fs2bNj6tSpccUVV8QxxxwT9913X/Tv3z/fSwUAAGhRQZZlWVsvYl9raGiI0tLSqK+v97E9AAA4gO1pG7zv7qoHAACwrwknAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQsE/C6ZZbbonevXtHx44dY8iQIfH444/vdvy8efOib9++0bFjxzj++OPjgQceyL22bdu2uOyyy+L444+Pgw8+OCoqKuLcc8+NDRs25PswAACAA1Tew2nu3LkxadKkmD59ejzxxBNxwgknxPDhw2Pjxo0tjn/00Udj9OjRMWHChFi+fHmMHDkyRo4cGStWrIiIiDfffDOeeOKJuPLKK+OJJ56Ie++9N9asWROf/vSn830oAADAAaogy7Isn28wZMiQOPHEE+Pmm2+OiIimpqaorKyMiy66KC6//PKdxp955pnxxhtvxPz583PbTjrppBgwYEDMnDmzxfdYsmRJDB48OJ5//vno1atXck0NDQ1RWloa9fX1UVJS8h6PDAAAeL/b0zbI6xWnrVu3xrJly6K6uvrPb9iuXVRXV0dNTU2Lc2pqapqNj4gYPnz4LsdHRNTX10dBQUGUlZW1yroBAAD+UmE+d/7yyy/H9u3bo2fPns229+zZM1avXt3inNra2hbH19bWtjh+y5Ytcdlll8Xo0aN3WYiNjY3R2NiYe97Q0LA3hwEAABzg3td31du2bVucccYZkWVZ3HbbbbscN2PGjCgtLc09Kisr9+EqAQCA97u8hlO3bt2iffv2UVdX12x7XV1dlJeXtzinvLx8j8bviKbnn38+FixYsNvPI06ZMiXq6+tzjxdeeOE9HhEAAHAgyms4FRUVxcCBA2PhwoW5bU1NTbFw4cKoqqpqcU5VVVWz8RERCxYsaDZ+RzStXbs2fv7zn0fXrl13u47i4uIoKSlp9gAAANhTef2OU0TEpEmTYuzYsTFo0KAYPHhw3HDDDfHGG2/E+PHjIyLi3HPPjcMPPzxmzJgRERGXXHJJDB06NK6//voYMWJEzJkzJ5YuXRq33357RLwTTZ/97GfjiSeeiPnz58f27dtz33/q0qVLFBUV5fuQAACAA0zew+nMM8+Ml156KaZNmxa1tbUxYMCAeOihh3I3gFi3bl20a/fnC18nn3xyzJ49O6ZOnRpXXHFFHHPMMXHfffdF//79IyJi/fr18b//+78RETFgwIBm7/WLX/wiPvaxj+X7kAAAgANM3n+P0/7I73ECAAAi9pPf4wQAAPBBIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAECCcAIAAEgQTgAAAAnCCQAAIEE4AQAAJAgnAACABOEEAACQIJwAAAAShBMAAEDCPgmnW265JXr37h0dO3aMIUOGxOOPP77b8fPmzYu+fftGx44d4/jjj48HHnig2etZlsW0adPisMMOi06dOkV1dXWsXbs2n4cAAAAcwPIeTnPnzo1JkybF9OnT44knnogTTjghhg8fHhs3bmxx/KOPPhqjR4+OCRMmxPLly2PkyJExcuTIWLFiRW7MtddeGzfddFPMnDkzFi9eHAcffHAMHz48tmzZku/DAQAADkAFWZZl+XyDIUOGxIknnhg333xzREQ0NTVFZWVlXHTRRXH55ZfvNP7MM8+MN954I+bPn5/bdtJJJ8WAAQNi5syZkWVZVFRUxOTJk+MrX/lKRETU19dHz549Y9asWXHWWWcl19TQ0BClpaVRX18fJSUlrXSkAADA+82etkFerzht3bo1li1bFtXV1X9+w3btorq6OmpqalqcU1NT02x8RMTw4cNz45999tmora1tNqa0tDSGDBmyy30CAAD8NQrzufOXX345tm/fHj179my2vWfPnrF69eoW59TW1rY4vra2Nvf6jm27GvNujY2N0djYmHve0NCwdwcCAAAc0A6Iu+rNmDEjSktLc4/Kysq2XhIAAPA+ktdw6tatW7Rv3z7q6uqaba+rq4vy8vIW55SXl+92/I5/7s0+p0yZEvX19bnHCy+88J6OBwAAODDlNZyKiopi4MCBsXDhwty2pqamWLhwYVRVVbU4p6qqqtn4iIgFCxbkxh911FFRXl7ebExDQ0MsXrx4l/ssLi6OkpKSZg8AAIA9ldfvOEVETJo0KcaOHRuDBg2KwYMHxw033BBvvPFGjB8/PiIizj333Dj88MNjxowZERFxySWXxNChQ+P666+PESNGxJw5c2Lp0qVx++23R0REQUFBfOlLX4pvfOMbccwxx8RRRx0VV155ZVRUVMTIkSPzfTgAAMABKO/hdOaZZ8ZLL70U06ZNi9ra2hgwYEA89NBDuZs7rFu3Ltq1+/OFr5NPPjlmz54dU6dOjSuuuCKOOeaYuO+++6J///65Mf/yL/8Sb7zxRpx33nmxadOm+H//7//FQw89FB07dsz34QAAAAegvP8ep/2R3+MEAABE7Ce/xwkAAOCDQDgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAICEvIXTq6++GmeffXaUlJREWVlZTJgwIV5//fXdztmyZUtMnDgxunbtGp07d45Ro0ZFXV1d7vWnnnoqRo8eHZWVldGpU6fo169f3Hjjjfk6BAAAgIjIYzidffbZsXLlyliwYEHMnz8/fvWrX8V555232zlf/vKX4/7774958+bFL3/5y9iwYUN85jOfyb2+bNmy6NGjR9x1112xcuXK+OpXvxpTpkyJm2++OV+HAQAAEAVZlmWtvdNVq1bFscceG0uWLIlBgwZFRMRDDz0Un/rUp+LFF1+MioqKnebU19dH9+7dY/bs2fHZz342IiJWr14d/fr1i5qamjjppJNafK+JEyfGqlWrYtGiRXu8voaGhigtLY36+vooKSl5D0cIAAB8EOxpG+TlilNNTU2UlZXloikiorq6Otq1axeLFy9ucc6yZcti27ZtUV1dndvWt2/f6NWrV9TU1Ozyverr66NLly6tt3gAAIB3KczHTmtra6NHjx7N36iwMLp06RK1tbW7nFNUVBRlZWXNtvfs2XOXcx599NGYO3du/OQnP9ntehobG6OxsTH3vKGhYQ+OAgAA4B17dcXp8ssvj4KCgt0+Vq9ena+1NrNixYo47bTTYvr06fHJT35yt2NnzJgRpaWluUdlZeU+WSMAAPDBsFdXnCZPnhzjxo3b7Zijjz46ysvLY+PGjc22v/322/Hqq69GeXl5i/PKy8tj69atsWnTpmZXnerq6naa88wzz8SwYcPivPPOi6lTpybXPWXKlJg0aVLueUNDg3gCAAD22F6FU/fu3aN79+7JcVVVVbFp06ZYtmxZDBw4MCIiFi1aFE1NTTFkyJAW5wwcODA6dOgQCxcujFGjRkVExJo1a2LdunVRVVWVG7dy5cr4xCc+EWPHjo1vfvObe7Tu4uLiKC4u3qOxAAAA75aXu+pFRJx66qlRV1cXM2fOjG3btsX48eNj0KBBMXv27IiIWL9+fQwbNizuuOOOGDx4cEREfPGLX4wHHnggZs2aFSUlJXHRRRdFxDvfZYp45+N5n/jEJ2L48OFx3XXX5d6rffv2exR0O7irHgAAELHnbZCXm0NERNx9991x4YUXxrBhw6Jdu3YxatSouOmmm3Kvb9u2LdasWRNvvvlmbtt3vvOd3NjGxsYYPnx43HrrrbnX77nnnnjppZfirrvuirvuuiu3/cgjj4znnnsuX4cCAAAc4PJ2xWl/5ooTAAAQ0ca/xwkAAOCDRDgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAIAE4QQAAJAgnAAAABKEEwAAQIJwAgAASBBOAAAACcIJAAAgQTgBAAAkCCcAAICEvIXTq6++GmeffXaUlJREWVlZTJgwIV5//fXdztmyZUtMnDgxunbtGp07d45Ro0ZFXV1di2NfeeWVOOKII6KgoCA2bdqUhyMAAAB4R97C6eyzz46VK1fGggULYv78+fGrX/0qzjvvvN3O+fKXvxz3339/zJs3L375y1/Ghg0b4jOf+UyLYydMmBAf+chH8rF0AACAZgqyLMtae6erVq2KY489NpYsWRKDBg2KiIiHHnooPvWpT8WLL74YFRUVO82pr6+P7t27x+zZs+Ozn/1sRESsXr06+vXrFzU1NXHSSSflxt52220xd+7cmDZtWgwbNixee+21KCsr2+P1NTQ0RGlpadTX10dJSclfd7AAAMD71p62QV6uONXU1ERZWVkumiIiqquro127drF48eIW5yxbtiy2bdsW1dXVuW19+/aNXr16RU1NTW7bM888E1dffXXccccd0a6dr2gBAAD5V5iPndbW1kaPHj2av1FhYXTp0iVqa2t3OaeoqGinK0c9e/bMzWlsbIzRo0fHddddF7169Yo//OEPe7SexsbGaGxszD1vaGjYi6MBAAAOdHt1yebyyy+PgoKC3T5Wr16dr7XGlClTol+/fjFmzJi9mjdjxowoLS3NPSorK/O0QgAA4INor644TZ48OcaNG7fbMUcffXSUl5fHxo0bm21/++2349VXX43y8vIW55WXl8fWrVtj06ZNza461dXV5eYsWrQonn766bjnnnsiImLH17O6desWX/3qV+Oqq65qcd9TpkyJSZMm5Z43NDSIJwAAYI/tVTh17949unfvnhxXVVUVmzZtimXLlsXAgQMj4p3oaWpqiiFDhrQ4Z+DAgdGhQ4dYuHBhjBo1KiIi1qxZE+vWrYuqqqqIiPjhD38Yb731Vm7OkiVL4nOf+1w88sgj0adPn12up7i4OIqLi/f4OAEAAP5SXr7j1K9fvzjllFPi85//fMycOTO2bdsWF154YZx11lm5O+qtX78+hg0bFnfccUcMHjw4SktLY8KECTFp0qTo0qVLlJSUxEUXXRRVVVW5O+q9O45efvnl3PvtzV31AAAA9kZewiki4u67744LL7wwhg0bFu3atYtRo0bFTTfdlHt927ZtsWbNmnjzzTdz277zne/kxjY2Nsbw4cPj1ltvzdcSAQAA9khefo/T/s7vcQIAACLa+Pc4AQAAfJAIJwAAgAThBAAAkCCcAAAAEoQTAABAgnACAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgAThBAAAkCCcAAAAEoQTAABAgnACAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgAThBAAAkCCcAAAAEoQTAABAgnACAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgAThBAAAkCCcAAAAEoQTAABAgnACAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgAThBAAAkCCcAAAAEoQTAABAgnACAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgAThBAAAkCCcAAAAEoQTAABAQmFbL6AtZFkWERENDQ1tvBIAAKAt7WiCHY2wKwdkOG3evDkiIiorK9t4JQAAwP5g8+bNUVpausvXC7JUWn0ANTU1xYYNG+KQQw6JgoKCtl4Ou9DQ0BCVlZXxwgsvRElJSVsvh/cB5wx7w/nC3nLOsLecM+8PWZbF5s2bo6KiItq12/U3mQ7IK07t2rWLI444oq2XwR4qKSnxw4a94pxhbzhf2FvOGfaWc2b/t7srTTu4OQQAAECCcAIAAEgQTuy3iouLY/r06VFcXNzWS+F9wjnD3nC+sLecM+wt58wHywF5cwgAAIC94YoTAABAgnACAABIEE4AAAAJwgkAACBBONFmXn311Tj77LOjpKQkysrKYsKECfH666/vds6WLVti4sSJ0bVr1+jcuXOMGjUq6urqWhz7yiuvxBFHHBEFBQWxadOmPBwB+1o+zpmnnnoqRo8eHZWVldGpU6fo169f3Hjjjfk+FPLklltuid69e0fHjh1jyJAh8fjjj+92/Lx586Jv377RsWPHOP744+OBBx5o9nqWZTFt2rQ47LDDolOnTlFdXR1r167N5yGwj7XmObNt27a47LLL4vjjj4+DDz44Kioq4txzz40NGzbk+zDYh1r758xfOv/886OgoCBuuOGGVl41rSKDNnLKKadkJ5xwQvbYY49ljzzySPahD30oGz169G7nnH/++VllZWW2cOHCbOnSpdlJJ52UnXzyyS2OPe2007JTTz01i4jstddey8MRsK/l45z5r//6r+ziiy/OHn744ez//u//sjvvvDPr1KlT9t3vfjffh0MrmzNnTlZUVJR973vfy1auXJl9/vOfz8rKyrK6uroWx//mN7/J2rdvn1177bXZM888k02dOjXr0KFD9vTTT+fGfOtb38pKS0uz++67L3vqqaeyT3/609lRRx2VvfXWW/vqsMij1j5nNm3alFVXV2dz587NVq9endXU1GSDBw/OBg4cuC8PizzKx8+ZHe69997shBNOyCoqKrLvfOc7eT4S3gvhRJt45plnsojIlixZktv24IMPZgUFBdn69etbnLNp06asQ4cO2bx583LbVq1alUVEVlNT02zsrbfemg0dOjRbuHChcPqAyPc585cuuOCC7OMf/3jrLZ59YvDgwdnEiRNzz7dv355VVFRkM2bMaHH8GWeckY0YMaLZtiFDhmRf+MIXsizLsqampqy8vDy77rrrcq9v2rQpKy4uzr7//e/n4QjY11r7nGnJ448/nkVE9vzzz7fOomlT+TpnXnzxxezwww/PVqxYkR155JHCaT/lo3q0iZqamigrK4tBgwbltlVXV0e7du1i8eLFLc5ZtmxZbNu2Laqrq3Pb+vbtG7169YqamprctmeeeSauvvrquOOOO6JdO6f4B0U+z5l3q6+vjy5durTe4sm7rVu3xrJly5r9Xbdr1y6qq6t3+XddU1PTbHxExPDhw3Pjn3322aitrW02prS0NIYMGbLb84f3h3ycMy2pr6+PgoKCKCsra5V103bydc40NTXFOeecE5deemkcd9xx+Vk8rcJ/VdImamtro0ePHs22FRYWRpcuXaK2tnaXc4qKinb6l0/Pnj1zcxobG2P06NFx3XXXRa9evfKydtpGvs6Zd3v00Udj7ty5cd5557XKutk3Xn755di+fXv07Nmz2fbd/V3X1tbudvyOf+7NPnn/yMc5825btmyJyy67LEaPHh0lJSWts3DaTL7OmWuuuSYKCwvj4osvbv1F06qEE63q8ssvj4KCgt0+Vq9enbf3nzJlSvTr1y/GjBmTt/egdbX1OfOXVqxYEaeddlpMnz49PvnJT+6T9wQ+mLZt2xZnnHFGZFkWt912W1svh/3UsmXL4sYbb4xZs2ZFQUFBWy+HhMK2XgAfLJMnT45x48btdszRRx8d5eXlsXHjxmbb33777Xj11VejvLy8xXnl5eWxdevW2LRpU7MrCHV1dbk5ixYtiqeffjruueeeiHjnjlgREd26dYuvfvWrcdVVV73HIyNf2vqc2eGZZ56JYcOGxXnnnRdTp059T8dC2+nWrVu0b99+p7tstvR3vUN5eflux+/4Z11dXRx22GHNxgwYMKAVV09byMc5s8OOaHr++edj0aJFrjZ9QOTjnHnkkUdi48aNzT4ls3379pg8eXLccMMN8dxzz7XuQfBXccWJVtW9e/fo27fvbh9FRUVRVVUVmzZtimXLluXmLlq0KJqammLIkCEt7nvgwIHRoUOHWLhwYW7bmjVrYt26dVFVVRURET/84Q/jqaeeiieffDKefPLJ+M///M+IeOcH08SJE/N45LxXbX3ORESsXLkyPv7xj8fYsWPjm9/8Zv4OlrwpKiqKgQMHNvu7bmpqioULFzb7u/5LVVVVzcZHRCxYsCA3/qijjory8vJmYxoaGmLx4sW73CfvH/k4ZyL+HE1r166Nn//859G1a9f8HAD7XD7OmXPOOSd++9vf5v675cknn4yKioq49NJL46c//Wn+Dob3pq3vTsGB65RTTsn+9m//Nlu8eHH261//OjvmmGOa3Vr6xRdfzD784Q9nixcvzm07//zzs169emWLFi3Kli5dmlVVVWVVVVW7fI9f/OIX7qr3AZKPc+bpp5/Ounfvno0ZMyb74x//mHts3Lhxnx4bf705c+ZkxcXF2axZs7JnnnkmO++887KysrKstrY2y7IsO+ecc7LLL788N/43v/lNVlhYmH3729/OVq1alU2fPr3F25GXlZVlP/7xj7Pf/va32WmnneZ25B8grX3ObN26Nfv0pz+dHXHEEdmTTz7Z7GdKY2NjmxwjrSsfP2fezV319l/CiTbzyiuvZKNHj846d+6clZSUZOPHj882b96ce/3ZZ5/NIiL7xS9+kdv21ltvZRdccEF26KGHZgcddFB2+umnZ3/84x93+R7C6YMlH+fM9OnTs4jY6XHkkUfuwyOjtXz3u9/NevXqlRUVFWWDBw/OHnvssdxrQ4cOzcaOHdts/A9+8IPsb/7mb7KioqLsuOOOy37yk580e72pqSm78sors549e2bFxcXZsGHDsjVr1uyLQ2Efac1zZsfPoJYef/lzife31v45827Caf9VkGV/+hIIAAAALfIdJwAAgAThBAAAkCCcAAAAEoQTAABAgnACAABIEE4AAAAJwgkAACBBOAEAACQIJwAAgAThBAAAkCCcAAAAEoQTAABAwv8Hwpn5/x95AngAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger[10000:])\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cb45361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(entropy_logger[10000:])\n",
    "_ = plt.title(\"Distribution Entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9196d8cc",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1184b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an example from the test set\n",
    "text = next(iter(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d69b9e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[354, 274, 354,  ...,   0,   0,   0],\n",
       "         [777, 528, 294,  ...,   0,   0,   0],\n",
       "         [657, 654, 653,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [111, 111,   0,  ...,   0,   0,   0],\n",
       "         [423, 921, 784,  ...,   0,   0,   0],\n",
       "         [693, 455, 302,  ...,   0,   0,   0]]),\n",
       " tensor([[274, 354,   0,  ...,   0,   0,   0],\n",
       "         [528, 294, 185,  ...,   0,   0,   0],\n",
       "         [654, 653, 652,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [111,   0,   0,  ...,   0,   0,   0],\n",
       "         [921, 784, 832,  ...,   0,   0,   0],\n",
       "         [455, 302, 820,  ...,   0,   0,   0]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "158dc22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354, 274, 354, ...,   0,   0,   0],\n",
       "       [777, 528, 294, ...,   0,   0,   0],\n",
       "       [657, 654, 653, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [111, 111,   0, ...,   0,   0,   0],\n",
       "       [423, 921, 784, ...,   0,   0,   0],\n",
       "       [693, 455, 302, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 0.6\n",
    "\n",
    "\n",
    "prompt = text[0]\n",
    "prompt.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961fa7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e60924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set index of the example to use\n",
    "# index = 0\n",
    "\n",
    "# # Set temperature for sampling\n",
    "# temp = 0.6\n",
    "\n",
    "# # Split text into title and content\n",
    "# title = text[index].split(\":\")[0]\n",
    "# init_prompt = [title + \":\"]  # Create initial prompt using the title\n",
    "\n",
    "# # Extract content from text\n",
    "# content = text[index].split(\":\")[1]\n",
    "\n",
    "# # Tokenize the initial prompt\n",
    "# init_tokens = gen_tranform(init_prompt)\n",
    "\n",
    "# # Print initial prompt, original content, and tokenized prompt\n",
    "# print(\"INITIAL PROMPT:\")\n",
    "# print(title)\n",
    "# print(\"\")\n",
    "# print(\"ORIGINAL CONTENT:\")\n",
    "# print(content)\n",
    "# print(\"\")\n",
    "# print(\"PROMPT TOKENS:\")\n",
    "# print(init_tokens)\n",
    "# print(vocab.lookup_tokens(init_tokens[0].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5fa929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0')\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0')\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0')\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0')\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0')\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0')\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0')\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0')\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0')\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# List to log generated tokens\n",
    "log_tokens = [prompt]\n",
    "\n",
    "# Set the generator model to evaluation mode\n",
    "tf_generator.eval()\n",
    "\n",
    "# Generate tokens\n",
    "with torch.no_grad():    \n",
    "    for i in range(10):\n",
    "        # Concatenate tokens from previous iterations\n",
    "        input_tokens = torch.cat(log_tokens, 1)\n",
    "        \n",
    "        # Get model predictions for the next token\n",
    "        data_pred = tf_generator(input_tokens.to(device))\n",
    "        print(data_pred) \n",
    "        # Sample the next token from the distribution of probabilities\n",
    "        # dist = Categorical(logits=data_pred[:, -1] / temp)\n",
    "        # next_tokens = dist.sample().reshape(1, 1)\n",
    "        \n",
    "        # # Append the sampled token to the list of generated tokens\n",
    "        # log_tokens.append(next_tokens.cpu())\n",
    "        \n",
    "        # # Check for end-of-sequence token and stop generation\n",
    "        # if next_tokens.item() == 2:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe6d73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate generated tokens into a single string\n",
    "pred_text = \"\".join(vocab.lookup_tokens(torch.cat(log_tokens, 1)[0].numpy()))\n",
    "\n",
    "# Print the generated text\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace special tokens and characters in the generated text\n",
    "pred_text_cleaned = pred_text.replace(\"‚ñÅ\", \" \").replace(\"<unk>\", \"\").replace(\"<sos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "# Print the cleaned generated text\n",
    "print(pred_text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the softmax probabilities of the next token\n",
    "_ = plt.plot(F.softmax(data_pred[0, -1] / temp, -1).cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b1fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-zYxsoNGC-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
