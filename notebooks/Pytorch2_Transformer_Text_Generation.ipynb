{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaab1d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a67dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-16 20:33:50.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrecsys.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /home/artem216/transformer-recsys\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from recsys import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5e339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Number of epochs for training\n",
    "nepochs = 100\n",
    "\n",
    "# Batch size for data loaders\n",
    "batch_size = 128\n",
    "\n",
    "# Maximum sequence length for text inputs\n",
    "max_len = 64\n",
    "\n",
    "# Root directory of the dataset\n",
    "data_set_root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803a8dc",
   "metadata": {},
   "source": [
    "## Data processing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08466ffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Un-Comment to train sentence-piece model for tokenizer and vocab!\n",
    "\n",
    "from torchtext.data.functional import generate_sp_model\n",
    "\n",
    "with open(os.path.join(data_set_root, \"datasets/AG_NEWS/train.csv\")) as f:\n",
    "    with open(os.path.join(data_set_root, \"datasets/AG_NEWS/data.txt\"), \"w\") as f2:\n",
    "        for i, line in enumerate(f):\n",
    "            text_only = \"\".join(line.split(\",\")[1:])\n",
    "            filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # remove newline characters\n",
    "            filtered = filtered.replace(' #39;', \"'\")\n",
    "            filtered = filtered.replace(' #38;', \"&\")\n",
    "            filtered = filtered.replace(' #36;', \"$\")\n",
    "            filtered = filtered.replace(' #151;', \"-\")\n",
    "\n",
    "            f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "generate_sp_model(os.path.join(data_set_root, \"datasets/AG_NEWS/data.txt\"), \n",
    "                  vocab_size=20000, model_prefix='spm_ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d2a0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGNews(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_train=\"train\"):\n",
    "        # Load the dataset from the specified CSV file\n",
    "        self.df = pd.read_csv(config.PROCESSED_DATA_DIR / f\"x_{test_train}_ids.csv\")\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve the article text and convert it to lowercase\n",
    "        text = self.df.loc[index][\"articles\"]\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return the number of data points in the dataset\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7087111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing datasets\n",
    "dataset_train = AGNews(test_train=\"train\")\n",
    "dataset_test = AGNews(test_train=\"test\")\n",
    "\n",
    "# Create data loaders for the training and testing datasets\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da5423e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'spm_ag_news.vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m [line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Build a vocabulary from the tokens yielded by the yield_tokens function\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# We will also add \"special\" tokens that we'll use to signal something to our model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# <pad> is a padding token that is added to the end of a sentence to ensure \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# <unk> \"unknown\" token is used if a token is not contained in the vocab\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vocab_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43myield_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspm_ag_news.vocab\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mspecials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<pad>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<sos>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<eos>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<unk>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mspecial_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Set the default index for unknown tokens to the index of the '<unk>' token\u001b[39;00m\n\u001b[1;32m     20\u001b[0m vocab\u001b[38;5;241m.\u001b[39mset_default_index(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/recsys-zYxsoNGC-py3.11/lib/python3.11/site-packages/torchtext/vocab/vocab_factory.py:98\u001b[0m, in \u001b[0;36mbuild_vocab_from_iterator\u001b[0;34m(iterator, min_freq, specials, special_first, max_tokens)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mBuild a Vocab from an iterator.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    >>> vocab = build_vocab_from_iterator(yield_tokens(file_path), specials=[\"<unk>\"])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m---> 98\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcounter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m specials \u001b[38;5;241m=\u001b[39m specials \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m, in \u001b[0;36myield_tokens\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myield_tokens\u001b[39m(file_path):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# Iterate through each line in the file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;66;03m# Yield the token from the first column (split by tab)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m [line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spm_ag_news.vocab'"
     ]
    }
   ],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate through each line in the file\n",
    "        for line in f:\n",
    "            # Yield the token from the first column (split by tab)\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"spm_ag_news.vocab\"), \n",
    "                                  specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "                                  special_first=True)\n",
    "\n",
    "# Set the default index for unknown tokens to the index of the '<unk>' token\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\"For a batch of tokens indices, randomly replace a non-specical token with <pad>.\n",
    "    \n",
    "    Args:\n",
    "        prob (float): probability of dropping a token\n",
    "        pad_token (int): index for the <pad> token\n",
    "        num_special (int): Number of special tokens, assumed to be at the start of the vocab\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n",
    "        self.prob = prob\n",
    "        self.num_special = num_special\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Randomly sample a bernoulli distribution with p=prob\n",
    "        # to create a mask where 1 means we will replace that token\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        \n",
    "        # only replace if the token is not a special token\n",
    "        can_drop = (sample >= self.num_special).long()\n",
    "        mask = mask * can_drop\n",
    "        \n",
    "        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n",
    "        \n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        \n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca059c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tranform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"spm_ag_news.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0),\n",
    ")\n",
    "\n",
    "gen_tranform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"spm_ag_news.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6eba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = next(iter(data_loader_train))\n",
    "index = 0\n",
    "input_tokens = train_tranform(list(text))\n",
    "print(\"SENTENCE\")\n",
    "print(text[index])\n",
    "print()\n",
    "print(\"TOKENS\")\n",
    "print(vocab.lookup_tokens(input_tokens[index].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68178f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOKENS BACK TO SENTENCE\")\n",
    "\n",
    "pred_text = \"\".join(vocab.lookup_tokens(input_tokens[index].numpy()))\n",
    "pred_text.replace(\"▁\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65424c",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effad54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal positional embeddings\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional embeddings module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate sinusoidal positional embeddings\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "    \n",
    "# Transformer block with Attention and causal masking\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with self-attention and causal masking.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=128, num_heads=4):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # Layer normalization for input\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Multi-head self-attention mechanism\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, \n",
    "                                                    num_heads=num_heads, \n",
    "                                                    batch_first=True,\n",
    "                                                    dropout=0.1)\n",
    "\n",
    "        # Layer normalization for attention output\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Feedforward neural network\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, padding_mask):\n",
    "        # Create causal mask for Attention\n",
    "        bs, l, h = x.shape\n",
    "        mask = torch.triu(torch.ones(l, l, device=x.device), 1).bool()\n",
    "\n",
    "        # Layer normalization\n",
    "        norm_x = self.norm1(x)\n",
    "\n",
    "        # Apply multi-head Attention\n",
    "        x = self.multihead_attn(norm_x, norm_x, norm_x, attn_mask=mask, key_padding_mask=padding_mask)[0] + x\n",
    "\n",
    "        # Layer normalization\n",
    "        norm_x = self.norm2(x)\n",
    "\n",
    "        # Apply feedforward neural network\n",
    "        x = self.mlp(norm_x) + x\n",
    "        return x\n",
    "\n",
    "    \n",
    "# \"Decoder-Only\" Style Transformer with Attention\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    \"Decoder-Only\" Style Transformer with self-attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Token embeddings\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "\n",
    "        # Positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "\n",
    "        # List of Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Mask for padding tokens\n",
    "        input_key_mask = input_seq == 0\n",
    "\n",
    "        # Embedding input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to token embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "\n",
    "        # Pass through Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs, padding_mask=input_key_mask)\n",
    "\n",
    "        # Output predictions\n",
    "        return self.fc_out(embs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc39c75",
   "metadata": {},
   "source": [
    "## Initialise Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(1 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Embedding Size\n",
    "hidden_size = 256\n",
    "\n",
    "# Number of transformer blocks\n",
    "num_layers = 8\n",
    "\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8\n",
    "\n",
    "# Create model\n",
    "tf_generator = Transformer(num_emb=len(vocab), num_layers=num_layers, \n",
    "                           hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "optimizer = optim.Adam(tf_generator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "# Custom transform that will randomly replace a token with <pad>\n",
    "td = TokenDrop(prob=0.2)\n",
    "\n",
    "# Initialize training loss logger and entropy logger\n",
    "training_loss_logger = []\n",
    "entropy_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcba694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in tf_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8b005",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9da7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):    \n",
    "    tf_generator.train()\n",
    "    steps = 0\n",
    "    for text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        # Convert text to tokenized input\n",
    "        text_tokens = train_tranform(list(text)).to(device)\n",
    "        bs = text_tokens.shape[0]\n",
    "        \n",
    "        # Randomly drop input tokens\n",
    "        input_text = td(text_tokens[:, 0:-1])\n",
    "        output_text = text_tokens[:, 1:]\n",
    "\n",
    "        # Generate predictions\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = tf_generator(input_text)\n",
    "\n",
    "        # Calculate loss with masked cross-entropy\n",
    "        mask = (output_text != 0).float()\n",
    "        loss = (loss_fn(pred.transpose(1, 2), output_text) * mask).sum()/mask.sum()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Log training loss and entropy\n",
    "        training_loss_logger.append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            dist = Categorical(logits=pred)\n",
    "            entropy_logger.append(dist.entropy().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82814571",
   "metadata": {},
   "source": [
    "## Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger[10000:])\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb45361",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(entropy_logger[10000:])\n",
    "_ = plt.title(\"Distribution Entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9196d8cc",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an example from the test set\n",
    "text = next(iter(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e60924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index of the example to use\n",
    "index = 0\n",
    "\n",
    "# Set temperature for sampling\n",
    "temp = 0.6\n",
    "\n",
    "# Split text into title and content\n",
    "title = text[index].split(\":\")[0]\n",
    "init_prompt = [title + \":\"]  # Create initial prompt using the title\n",
    "\n",
    "# Extract content from text\n",
    "content = text[index].split(\":\")[1]\n",
    "\n",
    "# Tokenize the initial prompt\n",
    "init_tokens = gen_tranform(init_prompt)\n",
    "\n",
    "# Print initial prompt, original content, and tokenized prompt\n",
    "print(\"INITIAL PROMPT:\")\n",
    "print(title)\n",
    "print(\"\")\n",
    "print(\"ORIGINAL CONTENT:\")\n",
    "print(content)\n",
    "print(\"\")\n",
    "print(\"PROMPT TOKENS:\")\n",
    "print(init_tokens)\n",
    "print(vocab.lookup_tokens(init_tokens[0].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to log generated tokens\n",
    "log_tokens = [init_tokens]\n",
    "\n",
    "# Set the generator model to evaluation mode\n",
    "tf_generator.eval()\n",
    "\n",
    "# Generate tokens\n",
    "with torch.no_grad():    \n",
    "    for i in range(10):\n",
    "        # Concatenate tokens from previous iterations\n",
    "        input_tokens = torch.cat(log_tokens, 1)\n",
    "        \n",
    "        # Get model predictions for the next token\n",
    "        data_pred = tf_generator(input_tokens.to(device))\n",
    "        \n",
    "        # Sample the next token from the distribution of probabilities\n",
    "        dist = Categorical(logits=data_pred[:, -1] / temp)\n",
    "        next_tokens = dist.sample().reshape(1, 1)\n",
    "        \n",
    "        # Append the sampled token to the list of generated tokens\n",
    "        log_tokens.append(next_tokens.cpu())\n",
    "        \n",
    "        # Check for end-of-sequence token and stop generation\n",
    "        if next_tokens.item() == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe6d73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate generated tokens into a single string\n",
    "pred_text = \"\".join(vocab.lookup_tokens(torch.cat(log_tokens, 1)[0].numpy()))\n",
    "\n",
    "# Print the generated text\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace special tokens and characters in the generated text\n",
    "pred_text_cleaned = pred_text.replace(\"▁\", \" \").replace(\"<unk>\", \"\").replace(\"<sos>\", \"\").replace(\"<eos>\", \"\")\n",
    "\n",
    "# Print the cleaned generated text\n",
    "print(pred_text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the softmax probabilities of the next token\n",
    "_ = plt.plot(F.softmax(data_pred[0, -1] / temp, -1).cpu().numpy().flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-zYxsoNGC-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
